---
title: "R Notebook"
output: html_notebook
---

```{r setup}
local <- TRUE

wd <- ""

if (!local) {
  # Cluster environment
  wd <- "~/Documents/Autocorrelation"
  # Base directory for the data set we will create
  dat_dir <- "/project_cephfs/3022017.02/projects/lorkno/data"
} else {
  wd <- "~/HPC/Documents/Autocorrelation"
  # Base directory for the data set we will create
  dat_dir <- "~/HPC_project/data"
}

# For some reason this sets the pwd only for the current scope, which means it 
# does not affect anything outside an if block if you set it there.
# So that's why it's here instead of above.
setwd(wd) 

source("src/load_dependencies.R")
```

# Clean and select self-report measures

```{r}
dat_rep_full <- read_sav(file.path(dat_dir, "clear3daily_20221205.sav"))
dat_rep_full <- zap_labels(dat_rep_full)
dat_rep_full
```

```{r}
unique(dat_rep_full$id)
```


```{r}
table(dat_rep_full$NSSIyn)
```

```{r}
colnames(dat_rep_full)
```


Add some summary measures first.

```{r}
dat_rep_summ <- dat_rep_full %>%
  mutate(
    suicidality_mean = rowMeans(select(., 
                                       ASIQ9_wishdead, 
                                       ASIQ2_thoughtkill, 
                                       ASIQ16_thoughtways, 
                                       ASIQ19_lifenotworth, 
                                       ASIQ1_betternotalive,
                                       ASIQ25_notbetterkill,
                                       ASIQ17_thoughtkillnotdo),
                                na.rm = TRUE),
    PMDDemosx_mean = rowMeans(select(.,
                                     DRSP1_depblue,
                                     DRSP2_hopeless,
                                     DRSP3_worthguilt,
                                     DRSP4_anxious,
                                     DRSP5_moodswings,
                                     DRSP6_rejsens,
                                     DRSP7a_angry,
                                     DRSP7b_irritable,
                                     DRSP8_intconflict),
                              na.rm = TRUE),
    environment_int = rowMeans(select(., 
                                      DRSP22_workint, 
                                      DRSP23_hobbsocint, 
                                      DRSP24_relint),
                               na.rm = TRUE),
    rumination = rowMeans(select(.,
                                 RNT_FEEL, 
                                 RNT_repetitiveness, 
                                 RNT_uncontrollable, 
                                 angrum),
                          na.rm = TRUE),
    agitation = rowMeans(select(., starts_with("BAM")), na.rm = TRUE),
    impulsivity = rowMeans(select(., 
                                  actedwothink, 
                                  imp_trouble, 
                                  imp_regret), 
                           na.rm = TRUE),
    irritability_mean = rowMeans(select(., starts_with("BITe")), na.rm = TRUE)) %>%
  rowwise() %>%
  mutate(
    n_stressors = case_when(
      all(is.na(c_across(stress_1:stress_15))) ~ NA_real_,
      TRUE ~ sum(c_across(stress_1:stress_15), na.rm = TRUE)
    )
  ) %>%
  ungroup()

dat_rep_summ
```

Filter out columns that are not suited for a regression analysis. Reasons for exclusion are given in the excel file.

```{r}
inclusion <- read_excel("regression_inclusion_20221205.xlsx")$Included
dat_rep_reg <- dat_rep_summ[inclusion == 1]
dat_rep_reg
```

```{r}
table(dat_rep_reg$menstrualbleeding, dat_rep_reg$firstdayofperiod, useNA = "ifany")
```


Change the encoding of some columns (e.g. to factors, or boolean). Likert scales will be left as-is. The mhpYN_[n] columns seem to require some special handling: When one of the variables has a value of 1, the others tend to have `NA`s. (Note that there are some cases where that assumption doesn't hold, as shown by a contingency table. If all mhpYN values are NA, I suspect there has been a data collection error.) I will convert `NA`s to 0 only if any of the other mhpYN variables do not have an NA value. As for firstdayofperiod, we can see that it is `NA` whenever menstrualbleeding is 0. I can understand why this is the case (because firstdayofperiod is not a useful measure if someone is not even on their period), but it messes with methods using complete case analysis, so I will change those `NA` cases to 0.

```{r}
# Replace NAs in partially complete cases
repl_part_comp <- function(col, value, ...) {
  other_cols <- list(...)
  n <- length(other_cols[[1]])
  
  case_when(
    is.na(col) & sapply(1:n, function(i) {
      !all(sapply(other_cols, function(col) is.na(col[i])))
    }) ~ value,
    TRUE ~ col
  )
}

dat_rep_reg <- dat_rep_reg %>%
  mutate(
    id = factor(id),
    firstdayofperiod = case_when(
      menstrualbleeding == 0 ~ 0,
      TRUE ~ firstdayofperiod
    ),
    workday = workday == 1,
    mhpYN_0 = repl_part_comp(mhpYN_0, value = 0, mhpYN_1, mhpYN_2),
    mhpYN_1 = repl_part_comp(mhpYN_1, value = 0, mhpYN_0, mhpYN_2),
    mhpYN_2 = repl_part_comp(mhpYN_2, value = 0, mhpYN_0, mhpYN_1),
    usedPRN = usedPRN == 1,
    NSSIyn = factor(NSSIyn),
    MJuse = MJuse == 1
  )

dat_rep_reg
```

As for the 05-12-2022 snapshot of the data set, we see many spurious days for a couple of subjects (with `daterated` already starting in 1990). Those days contain no actual data, so that makes it easier to exclude them from the data set.

```{r}
all_missing <- data.frame(is.na(dat_rep_reg)) %>%
  rowwise() %>%
  summarize(allMissing = all(c_across(menstrualbleeding:panicattack)))

dat_rep_reg <- dat_rep_reg[!all_missing$allMissing,]
dat_rep_reg
```

```{r warning=FALSE}
max.print <- options("max.print")$max.print
options(max.print = 10 * max.print)

tab1 <- CreateTableOne(data = dat_rep_reg)
print(summary(tab1))

options(max.print = max.print)
```

# Join with TDIAs

Temporarily add daterated back in to allow joining.

```{r}
# dat_rep_reg$daterated <- dat_rep_full$daterated
# dat_rep_reg %>%
#   relocate(daterated, .after = id)
```

Load TDIA data.

```{r}
load(file.path(dat_dir, "tdias.rda"))
```

Load subject key press data.

```{r}
dirs <- list.dirs(dat_dir)

pattern <- "sub-([0-9]+)/preproc$"
preproc_paths <- str_subset(dirs, pattern)
subjects <- str_match(preproc_paths, pattern)[,2]
```

Remove subjects with <30 days of data.

```{r}
# Vector of indices for all eligible subjects
sub_idx <- 1:length(subjects)

load(file.path(dat_dir, "dats_kp.rda"))
load(file.path(dat_dir, "dats_ses.rda"))

for (i in sub_idx) {
  if (n_distinct(dats_ses[[i]]$date) < 30) {
    # Remove subject index
    sub_idx <- sub_idx[sub_idx != i]
  }
}

subjects <- subjects[sub_idx]
dats_kp <- dats_kp[sub_idx]
dats_ses <- dats_ses[sub_idx]
```

## Join the dates and TDIAs on subject

```{r}
sub_tdia <- link_tdia_date(tdias, subjects, dats_kp)
sub_tdia
```

## Join TDIA with self-report measures based on date and subject

```{r}
dat_reg <- sub_tdia %>%
  full_join(dat_rep_reg, by = c("subject" = "id", "date" = "daterated"))

dat_reg
```

```{r warning=FALSE}
max.print <- options("max.print")$max.print
options(max.print = 10 * max.print)

tab1 <- CreateTableOne(data = dat_reg)
print(summary(tab1))

options(max.print = max.print)
```

```{r}
sum(complete.cases(dat_reg))
```

Save (unscaled) cleaned data to file.

```{r}
save(dat_reg, file = file.path(dat_dir, "dat_reg.rda"))
```

Scaling continuous variables (except mean_accuracy and date).

```{r}
should.rescale <- sapply(dat_reg, typeof) == "double"
should.rescale["mean_accuracy"] <- FALSE
should.rescale["date"] <- FALSE

dat_reg_scaled <- dat_reg
dat_reg_scaled[,should.rescale] <- scale(dat_reg_scaled[,should.rescale])
dat_reg_scaled
```

# Number of eligible subjects

```{r}
unique(dat_rep_full$id)
```


```{r}
all_ids <- sort(base::union(dat_rep_reg$id, sub_tdia$subject))
df <- data.frame(subject = all_ids, 
                 in_selfrep = all_ids %in% dat_rep_reg$id,
                 in_tdia = all_ids %in% sub_tdia$subject)
df
```

```{r}
sum(df$in_selfrep & df$in_tdia)
```

How many subjects are in tdia that are not in selfrep?

```{r}
sum(df$in_tdia & !df$in_selfrep)
```


# Predicting TDIA

```{r}
m1 <- glmer(mean_accuracy ~ . - subject - partition_idx - date + (1 | subject), 
            data = dat_reg_scaled,
            family = "binomial")
summary(m1)
```

Too many predictors, most likely. We need to be more selective about which variables we use.
Let's go for the aggregates first.

```{r}
m2 <- glmer(mean_accuracy ~ suicidality_mean + 
              PMDDemosx_mean +
              environment_int +
              rumination +
              agitation +
              impulsivity +
              irritability_mean +
              n_stressors +
              (1 | subject), 
            data = dat_reg_scaled,
            family = "binomial")
summary(m2)
```

Hm, still convergence issues, and nothing is significant. Perhaps more iterations?

```{r}
ss <- getME(m2, c("theta", "fixef"))
m2a <- update(m2, start = ss,
              control = glmerControl(optCtrl = list(maxfun = 2e4)))
summary(m2a)
```

That helped solve the convergence issue, but nothing here is significant. Perhaps it's better to use accuracy as the predictor rather than the dependent variable. Not that the self-report variables are very normally distributed:

```{r}
hist(dat_reg_scaled$DRSPx_panicked)
```

From predicting the self-report measures, we find several variables that could be interesting. Let's see if we can plug them into a single model:

```{r}
m3 <- glmer(mean_accuracy ~ numdrinks_yest +
              DRSPx_worried +
              DRSPx_afraid +
              DRSPx_notenjoy +
              DRSP17_outofcontrol +
              PANAS_happy +
              mastery +
              suicidality_mean +
              PMDDemosx_mean +
              (1 | subject),
            data = dat_reg_scaled,
            family = "binomial",
            verbose = 0,
            control = glmerControl("nloptwrap"))
summary(m3)
```

No luck. Try to re-fit:

```{r}
m3.all <- allFit(m3, data = dat_reg_scaled)
summary(m3.all)
```

Still no luck.

## Predicting TDIA with only numdrinks_yest

```{r}
m4 <- glmer(mean_accuracy ~ numdrinks_yest + (1 | subject),
            data = dat_reg_scaled,
            family = "binomial")
summary(m4)
```

# Predicting the self-report measures

```{r paged.print=FALSE}
# Indices of self-report columns
rep_var_idx <- 5:ncol(dat_reg_scaled)

models <- vector("list", length(rep_var_idx))
accuracies <- as.vector(dat_reg_scaled$mean_accuracy)
sub_codes <- as.vector(dat_reg_scaled$subject)
col_names <- colnames(dat_reg_scaled)

for (i in 1:length(rep_var_idx)) {
  idx <- rep_var_idx[i]
  
  rep_var <- as.vector(dat_reg_scaled[[idx]])
  
  if (typeof(rep_var) != "double") {
    print("Not numeric, skipping.")
    next
  }
  
  models[[i]] <- lme(rep_var ~ accuracies, random = ~ 1 | sub_codes,
                     correlation = corAR1(), na.action = na.omit)
  # models[[i]] <- lme(rep_var ~ accuracies, random = ~ 1 | sub_codes,
  #                    na.action = na.omit)
  
  summ <- summary(models[[i]])
  t_val <- summ$tTable[2,4]
  if (abs(t_val) > 2) {
    print(col_names[idx])
    print(summary(models[[i]]))
    cat("\n-----------------------------\n\n")
  }
}
```

```{r}
for (i in rep_var_idx) {
  if (typeof(dat_reg_scaled[[i]]) != "double")
    next
  
  hist(dat_reg_scaled[[i]], main = col_names[i])
}
```

```{r}
table(dat_reg_scaled$numdrinks_yest)
```


It kind of looks like some variables might be gamma-distributed (especially the summary ones). To create gamma models, we must de-scale our variables, primarily because the gamma distribution is undefined over negative values.

```{r}
dat_reg_unscaled <- sub_tdia %>%
  full_join(dat_rep_reg, by = c("subject" = "id", "date" = "daterated"))

dat_reg_unscaled
```

```{r}
table(dat_reg_unscaled$numdrinks_yest)
```


```{r}
# Indices of self-report columns
rep_var_idx <- 5:ncol(dat_reg_unscaled)

models_gamma <- vector("list", length(rep_var_idx))
accuracies <- as.vector(dat_reg_unscaled$mean_accuracy)
sub_codes <- as.vector(dat_reg_unscaled$subject)
col_names <- colnames(dat_reg_unscaled)

for (i in 1:length(rep_var_idx)) {
  idx <- rep_var_idx[i]
  print(col_names[idx])
  
  if (typeof(rep_var) != "double") {
    print("Not numeric, skipping.")
    next
  }
  
  tryCatch({
    rep_var <- as.vector(dat_reg_unscaled[[idx]]) + 0.001
    
    models_gamma[[i]] <- glmer(rep_var ~ accuracies + (1 | sub_codes), 
                               family = Gamma(link = "log"),
                               control = glmerControl("bobyqa"))
  }, error = function(cond) {
    message(paste0(cond, "\n"))
    message(paste0("Skipping ", col_names[idx], "\n"))
  })
  
  print(summary(models_gamma[[i]]))
}
```

## Predict numdrinks_yest with Poisson

```{r}
mPois <- glmer(numdrinks_yest ~ mean_accuracy + (1 | subject),
               data = dat_reg_unscaled,
               family = poisson())
summary(mPois)
```

```{r}
y <- na.omit(dat_reg_unscaled$numdrinks_yest[!is.na(dat_reg_unscaled$mean_accuracy)])
y_hat <- fitted(mPois)
plot(y, y_hat)
```

```{r}
print(mean(dat_reg_unscaled$numdrinks_yest[!is.na(dat_reg_unscaled$mean_accuracy)], na.rm = TRUE))
print(var(dat_reg_unscaled$numdrinks_yest[!is.na(dat_reg_unscaled$mean_accuracy)], na.rm = TRUE))
```

I believe we have some overdispersion. Perhaps better to model this with a quasi-poisson distribution, but lmer seems to be unable to handle that.

```{r}
fixef(mPois)
```


```{r}
coeff <- fixef(mPois)

lambda_low_acc <- exp(coeff[["(Intercept)"]])
lambda_high_acc <- exp(coeff[["(Intercept)"]] + coeff[["mean_accuracy"]])

x <- 0:10
df <- data.frame(numdrinks_yest = rep(x, times = 2), 
                 density = c(dpois(x, lambda_low_acc), dpois(x, lambda_high_acc)), 
                 accuracy = rep(c("Low", "High"), each = length(x)))

ggplot(df, aes(numdrinks_yest, density)) +
  geom_line(aes(color = accuracy))

ggsave("images/regression/poisson.png")
```

```{r}
counts <- dat_reg_unscaled %>%
  mutate(high_acc = mean_accuracy >= 0.5) %>%
  count(high_acc, numdrinks_yest) %>%
  drop_na() %>%
  add_row(high_acc = TRUE, numdrinks_yest = 9, n = 0, .before = 21) %>%
  group_by(high_acc) %>%
  mutate(n = n / max(n))

df$count <- counts$n / max(counts$n)

ggplot(df, aes(numdrinks_yest)) +
  geom_line(aes(y = density, color = accuracy)) +
  geom_point(aes(y = count, color = accuracy), alpha = 0.5) +
  ylab("Density or Normalised count")

ggsave("images/regression/poisson.png")
```


## Misc

```{r}
means <- dat_reg_unscaled %>%
  group_by(subject) %>%
  select(is.numeric) %>%
  summarize(across(menstrualbleeding:n_stressors, ~ mean(.x, na.rm = TRUE)))
variances <- dat_reg_unscaled %>%
  group_by(subject) %>%
  select(is.numeric) %>%
  summarize(across(menstrualbleeding:n_stressors, ~ var(.x, na.rm = TRUE)))
```

```{r}
variances[[2]]
```


```{r}
hist(dat_reg_unscaled$mean_accuracy, main = "mean_accuracy")

j <- 2

for (i in rep_var_idx) {
  if (typeof(dat_reg[[i]]) != "double")
    next
  
  par(mfrow = c(1, 2))
  
  y <- dat_reg_unscaled[[i]]
  hist(log(y), main = str_glue("log({col_names[i]})"))
  plot(means[[j]], sqrt(variances[[j]]))
  lines(0:5, 0:5)
  
  j <- j + 1
}
```

```{r}
hist(exp(dat_reg_unscaled$mean_accuracy))
```


```{r}
for (i in rep_var_idx) {
  print(str_glue("{i}: {col_names[i]}"))
}
```

```{r}
promising_idx <- c(7, 15, 16, 17, 21, 29, 31, 43, 47)

for (idx in promising_idx) {
  print(plot(models_gamma[[idx - 4]], main = col_names[idx]))
}
```




```{r}
m1 <- models_gamma[[47 - 4]]
shape <- 1 / sigma(m1)^2
scale <- cumsum(unlist(fixef(m1))) / shape

x <- seq(1, 5, 0.01)
plot(x, dgamma(x, shape, scale = scale[1]), type = "l")
```




